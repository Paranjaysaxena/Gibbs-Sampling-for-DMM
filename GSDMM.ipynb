{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GSDMM1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2ohB7c1IHa"
      },
      "source": [
        "from numpy.random import multinomial\n",
        "from numpy import log, exp\n",
        "\n",
        "class GibbsSamplingDMM:\n",
        "    def __init__(self, K=300, D=1000, alpha=0.1, beta=0.1, n_iters=10):\n",
        "        self.K = K\n",
        "        self.D = D\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "        # slots for computed variables\n",
        "        self.number_docs = None\n",
        "        self.vocab_size = None\n",
        "        self.cluster_doc_count = [0 for _ in range(K)]\n",
        "        self.cluster_word_count = [0 for _ in range(K)]\n",
        "        self.cluster_word_distribution = [{} for i in range(K)]\n",
        "        self.document_word_distribution = [{} for i in range(D)]\n",
        "        self.iterations_document_cluster_distribution = [[] for i in range(n_iters)]\n",
        "\n",
        "    @staticmethod\n",
        "    def _sample(p):\n",
        "  \n",
        "        return [i for i, entry in enumerate(multinomial(1, p)) if entry != 0][0]\n",
        "\n",
        "    def fit(self, docs, vocab_size):\n",
        "\n",
        "        alpha, beta, K, n_iters, V = self.alpha, self.beta, self.K, self.n_iters, vocab_size\n",
        "\n",
        "        D = len(docs)\n",
        "        self.number_docs = D\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # unpack to easy var names\n",
        "        m_z, n_z, n_z_w, n_d_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution, self.document_word_distribution\n",
        "        cluster_count = K\n",
        "        i_d_z = self.iterations_document_cluster_distribution\n",
        "        d_z = [None for i in range(len(docs))]\n",
        "\n",
        "        # initialize the clusters\n",
        "        for i, doc in enumerate(docs):\n",
        "\n",
        "            # choose a random  initial cluster for the doc\n",
        "            z = self._sample([1.0 / K for _ in range(K)])\n",
        "            d_z[i] = z\n",
        "            m_z[z] += 1\n",
        "            n_z[z] += len(doc) \n",
        "\n",
        "            for word in doc:\n",
        "                if word not in n_d_w[i]:\n",
        "                    n_d_w[i][word] = 0\n",
        "                n_d_w[i][word] += 1\n",
        "\n",
        "            for word in doc:\n",
        "                if word not in n_z_w[z]:\n",
        "                    n_z_w[z][word] = 0\n",
        "                n_z_w[z][word] += 1\n",
        "\n",
        "        for _iter in range(n_iters):\n",
        "            total_transfers = 0\n",
        "\n",
        "            for i, doc in enumerate(docs):\n",
        "\n",
        "                # remove the doc from it's current cluster\n",
        "                z_old = d_z[i]\n",
        "\n",
        "                m_z[z_old] -= 1\n",
        "                n_z[z_old] -= len(doc)\n",
        "\n",
        "                for word in doc:\n",
        "                    n_z_w[z_old][word] -= 1\n",
        "\n",
        "                    # compact dictionary to save space\n",
        "                    if n_z_w[z_old][word] == 0:\n",
        "                        del n_z_w[z_old][word]\n",
        "\n",
        "                # draw sample from distribution to find new cluster\n",
        "                p = self.score(doc, i)\n",
        "                z_new = self._sample(p)\n",
        "\n",
        "                # transfer doc to the new cluster\n",
        "                if z_new != z_old:\n",
        "                    total_transfers += 1\n",
        "\n",
        "                d_z[i] = z_new\n",
        "                i_d_z[_iter].append(d_z[i])\n",
        "                m_z[z_new] += 1\n",
        "                n_z[z_new] += len(doc)\n",
        "\n",
        "                for word in doc:\n",
        "                    if word not in n_z_w[z_new]:\n",
        "                        n_z_w[z_new][word] = 0\n",
        "                    n_z_w[z_new][word] += 1\n",
        "\n",
        "            \n",
        "            cluster_count_new = sum([1 for v in m_z if v > 0])\n",
        "            print(\"In stage %d: transferred %d clusters with %d clusters populated\" % (\n",
        "            _iter, total_transfers, cluster_count_new))\n",
        "            if total_transfers == 0 and cluster_count_new == cluster_count and _iter>5:\n",
        "                print(\"Converged.  Breaking out.\")\n",
        "                break\n",
        "            cluster_count = cluster_count_new\n",
        "\n",
        "        self.cluster_word_distribution = n_z_w\n",
        "        return d_z\n",
        "\n",
        "    def score(self, doc, index):\n",
        "    \n",
        "        alpha, beta, K, V, D = self.alpha, self.beta, self.K, self.vocab_size, self.number_docs\n",
        "        m_z, n_z, n_z_w, n_d_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution, self.document_word_distribution\n",
        "\n",
        "        p = [0 for _ in range(K)]\n",
        "\n",
        "        #  We break the formula into the following pieces\n",
        "        #  p = N1*N2/(D1*D2) = exp(lN1 - lD1 + lN2 - lD2)\n",
        "        #  lN1 = log(m_z[z] + alpha)\n",
        "        #  lD1 = log(D - 1 + K*alpha)\n",
        "        #  lN2 = log(product(product(n_z_w[w] + beta + j - 1))) \n",
        "        #      = sum(log(product(n_z_w[w] + beta + j - 1)))\n",
        "        #      = sum(sum(log(n_z_w[w] + beta + j  - 1)))\n",
        "        #  lD2 = log(product(n_z[d] + V*beta + i -1)) \n",
        "        #      = sum(log(n_z[d] + V*beta + i -1))\n",
        "\n",
        "        lD1 = log(D - 1 + K * alpha)\n",
        "        doc_size = len(doc)\n",
        "        for label in range(K):\n",
        "            lN1 = log(m_z[label] + alpha)\n",
        "            lN2 = 0\n",
        "            lD2 = 0\n",
        "            for word in doc:\n",
        "              temp = 0\n",
        "              for j in range(n_d_w[index][word]):\n",
        "                temp += log(n_z_w[label].get(word, 0) + beta + j)\n",
        "              lN2 += temp\n",
        "            for i in range(1, doc_size +1):\n",
        "                lD2 += log(n_z[label] + V * beta + i - 1)\n",
        "            p[label] = exp(lN1 - lD1 + lN2 - lD2)\n",
        "\n",
        "        # normalize the probability vector\n",
        "        pnorm = sum(p)\n",
        "        pnorm = pnorm if pnorm>0 else 1\n",
        "        return [pp/pnorm for pp in p]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}